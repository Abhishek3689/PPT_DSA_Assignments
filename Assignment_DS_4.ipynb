{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e254abea-15cd-42dc-99c2-33704d8868af",
   "metadata": {},
   "source": [
    "1. What is the purpose of the General Linear Model (GLM)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3346aaf9-ac54-4008-aea5-a2003f553728",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The purpose of the General Linear Model (GLM) is to analyze and model the relationship between a dependent variable and one or more independent variables\n",
    "## The GLM assumes that the dependent variable is continuous and follows a normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b382777-874b-4137-9ffd-b06840ef0f0f",
   "metadata": {},
   "source": [
    "2. What are the key assumptions of the General Linear Model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a3b5d91-0327-4ac8-a510-cbdaf1c2e09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assumption of Linear Model\n",
    "## 1.Linear Relationship: The relationship between the dependent variable and the independent variables is linear. \n",
    "## 2. Noramlity: The errors follow a normal distribution\n",
    "## 3. No multicollinearity: The independent variables are not highly correlated with each other\n",
    "## 4. Independence: The observations are independent of each other.\n",
    "## 5. Homoscedasticity: The variance of the errors is constant across all levels of the independent variables.\n",
    "## 6.Autocorrelation: There should not be any residual pattern "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784711a4-cd4c-438e-b7d3-9717931a7adc",
   "metadata": {},
   "source": [
    "3. How do you interpret the coefficients in a GLM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5168c6e-4fd2-44d3-bf2c-ee60d5428334",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Interpretation of Coefficients\n",
    "## 1 Magnitude:The magnitude of the coefficient indicates the size of the effect. For example, in linear regression, a coefficient of 0.5 means that for every\n",
    "# one-unit increase in the independent variable, the dependent variable is expected to increase by 0.5 units\n",
    "## 2. Sign:The sign of the coefficient (positive or negative) indicates the direction of the effect. Positive coefficients suggest a positive association\n",
    "## between the independent variable and the dependent variable, while negative coefficients suggest a negative association."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2123e0-b102-4190-a631-2d4c154522fc",
   "metadata": {},
   "source": [
    "4. What is the difference between a univariate and multivariate GLM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5566b70e-15d8-4669-a66a-5a0206dd634e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## In univariate there is only one dependent  variable to interpret while in mulitvariate there are more than one dependant variables and multiple independant variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e050ec89-0b20-44ee-839e-659927c02ac2",
   "metadata": {},
   "source": [
    "5. Explain the concept of interaction effects in a GLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b33dc397-ec10-436d-8f2b-34a27e1e85c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## In a General Linear Model (GLM), interaction effects refer to the combined influence or relationship between two or more independent variables on the dependent variable.\n",
    "## When there is an interaction effect, the effect of one independent variable on the dependent variable is not consistent across different levels or values of another independent variable.\n",
    "## In other words, the relationship between the predictors and the outcome depends on the combination of their values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfbcb3a-958b-4260-9efd-33aa67e3db2c",
   "metadata": {},
   "source": [
    "6. How do you handle categorical predictors in a GLM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04491b58-9887-4ce5-b293-8570ce48a54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## There are different ways to handle categorical predictors\n",
    "## Label Encoding: converting into numercial values that are distinct to each other and ranges from 0 to n-1 classes\n",
    "## One Hot Encoding: it converts into numerical values of 0 and 1 where for each distinct category has 1 and other are 0\n",
    "## Ordinal Encoding: This encoding involves conversion into numerical based on ranking\n",
    "## Binary Encoding: If there are only 2 category then we can convert to 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee878a5-1770-41e2-add6-02ae4a024ed0",
   "metadata": {},
   "source": [
    "7. What is the purpose of the design matrix in a GLM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b821c867-8ee5-4f6e-be62-cea870b53779",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The design matrix in a General Linear Model (GLM) serves as a crucial component for organizing and representing the predictor variables in a structured format\n",
    "## The design matrix is constructed by arranging the predictor variables, both continuous and categorical, in a matrix format where each row represents\n",
    "## an observation and each column represents a predictor variable or a combination of predictor variables (e.g., interaction terms).\n",
    "## It provides a concise and organized representation of the predictors for model estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a459b46-1e6f-48e6-8354-1471677c0e5d",
   "metadata": {},
   "source": [
    "8. How do you test the significance of predictors in a GLM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fa51c94-a094-45a8-8a33-4f0cb2971341",
   "metadata": {},
   "outputs": [],
   "source": [
    "## To test the significance of predictors we perform Hypothesis Test to check our assumptions by calcuating the Statistical Valu and p value\n",
    "## If p-value is smaller than indicative threshold than we reject the null hypothesis or if statistical value is greather than criitical value we reject the null hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2911d145-3cd9-41e4-b69b-14df27f54cac",
   "metadata": {},
   "source": [
    "9. What is the difference between Type I, Type II, and Type III sums of squares in a GLM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73b0fb3d-6372-46d1-9806-7387c25f3bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Type I Sums of Squares are Sequential, so the order of variables in the models makes a difference. This is rarely what we want in practice\n",
    "## Type II Sums of Squares should be used if there is no interaction between the independent variables\n",
    "## Like Type II, the Type III Sums of Squares are not sequential, so the order of specification does not matter\n",
    "## Unlike Type II, the Type III Sums of Squares do specify an interaction effect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8efff5-ba87-4457-888d-dd3e83a388df",
   "metadata": {},
   "source": [
    "10. Explain the concept of deviance in a GLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9425222a-565c-40eb-a7f9-f55a1925dfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Deviance is a goodness-of-fit metric for statistical models, particularly used for GLMs. It is defined as the difference between\n",
    "## the Saturated and Proposed Models and can be thought as how much variation in the data does our Proposed Model account for. Therefore, the lower the deviance, the better the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec5f2f5-81c8-45cc-8b38-4d723825c167",
   "metadata": {},
   "source": [
    "11. What is regression analysis and what is its purpose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b319c73c-26aa-41e0-a99d-5cccd70e010c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Regression Analysis is anlaysis where there is a Linear Relationship between dependent variable and independent variables.\n",
    "## The purpose of regression analysis is to predict continuous numerical variable based on some indpendent features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3602a531-15aa-4869-bdbd-2d64d085cd32",
   "metadata": {},
   "source": [
    "12. What is the difference between simple linear regression and multiple linear regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1eb5db87-5354-4793-8f20-ad16188f390d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## In Simple Linear Regression there is only one independent Feature and one dependent Feature whereas in multiple Linear Regression there are more than one independent Featrues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca15afb-95ed-4a43-af11-e39123cbad18",
   "metadata": {},
   "source": [
    "13. How do you interpret the R-squared value in regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59730116-ebb3-4be9-8f9c-7d3a65bed7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## R-squared is a goodness-of-fit measure for linear regression models. This statistic indicates the percentage of the variance in the dependent variable that the independent variables \n",
    "## explain collectively. R-squared measures the strength of the relationship between your model and the dependent variable on a convenient 0 â€“ 100% scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a4b4a4-2639-4696-8914-cef53ccf6f66",
   "metadata": {},
   "source": [
    "14. What is the difference between correlation and regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea20da59-165d-491c-bfbf-3c37cba34d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Correlation: It referes to the degree of relation between two variables and its value range between -1 to 1 .\n",
    "## +1 indicates most postively related and -1 indicates most negatively correlated and 0 indicaes no relation is there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd55cca3-4c2c-438b-8289-4a48af1a9f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Regression: It shows how one variable affects another. Here There is difference in Independent and Dependent Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1535d2db-01af-4784-b0f4-e320718d83ee",
   "metadata": {},
   "source": [
    "15. What is the difference between the coefficients and the intercept in regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec1251a6-6cdf-43a2-8bd5-4db01957044f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Coefficients: It represents the slope of the regression line whereas intercept represents the intersection point of Regression line with y axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f60406a-8b69-455e-bc72-3e45973d26de",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The slope of the regression line tells us how much the dependent variable changes when the independent variable changes by one unit.\n",
    "## For example, if the slope of the regression line is 0.5, then for every one unit increase in the independent variable,the dependent variable will increase by 0.5 units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85458f0a-f6cc-4280-842b-345bdf06c6e0",
   "metadata": {},
   "source": [
    "16. How do you handle outliers in regression analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1975543-fa12-4c17-959d-1dce75994d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## There are multiple ways to handle outliers depending upon the problem at hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4f48780-1ff0-43a2-8b52-3f4bc6908a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Removal: This is the simplest approach, but it can also be the most disruptive. If you remove too many outliers, \n",
    "## you may end up with a biased sample that does not accurately represent the population.\n",
    "\n",
    "## 2. Transform the data. This can be a good way to handle outliers without removing them from the data. There are a number of different transformations that can be used,\n",
    "## such as log transformation, square root transformation\n",
    "\n",
    "## 3. Use a robust regression model. Robust regression models are designed to be less sensitive to outliers than traditional regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ddd849-f7ca-4adb-852f-a8e2f4f76599",
   "metadata": {},
   "source": [
    "17. What is the difference between ridge regression and ordinary least squares regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8a62bb7-4479-4267-99df-bd3e577c11ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ridge regression minimizes the sum of squared residuals plus a penalty term that is proportional to the square of the coefficients. \n",
    "## This penalty term shrinks the coefficients towards zero, which can help to improve the stability and prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7696027-adaa-460e-b0dc-23eb1977fa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ordinary least squares regression (OLS) minimizes the sum of squared residuals, which is the difference between the observed and predicted values of the dependent variable. \n",
    "## This can lead to models with large coefficients, which can be unstable and overfit the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd6b05e-0120-4895-811b-90925b1a18e2",
   "metadata": {},
   "source": [
    "18. What is heteroscedasticity in regression and how does it affect the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1cfb0a9-f993-4cf2-b545-0da30c184f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Heteroscedasticity is a statistical phenomenon in which the variance of the residuals of a regression model is not constant across the range of fitted values.\n",
    "## This means that the spread of the residuals is not the same for all values of the independent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c851fb93-f8c0-4f6e-89e9-dfacee81ba76",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Heteroscedasticity can affect the validity of the regression model in a number of ways.It can make the standard errors of the coefficients inaccurate.\n",
    "## Second, heteroscedasticity can make the t-tests for the coefficients invalid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6db692-53bf-49b4-b76c-aae3afd7445a",
   "metadata": {},
   "source": [
    "19. How do you handle multicollinearity in regression analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b7d3d1ef-6721-4510-a9b9-23a2c0b3dddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## If there is Multicollinearity , Meaning there is high correlation between two or more than two independent features than its better to remove these features.\n",
    "## To overcome this problem we need to remove highly correlated variables.\n",
    "## Another approach is to use More Robust model which are less sesitive to Multicollinearity.\n",
    "## We can rescale the features to avoid multicollinearity\n",
    "## We can also use regularizaion technique like Lasso and Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1650038-9488-49b5-9c36-d8126a16340b",
   "metadata": {},
   "source": [
    "20. What is polynomial regression and when is it used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "130a56fe-0b74-4d43-a9a0-c0f8b361769c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Polynomial regression is a type of regression analysis in which the relationship between the independent variable x and the dependent variable y is modeled as a polynomial function.\n",
    "## The degree of the polynomial can be any positive integer, but it is most commonly 2, 3, or 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "702a90cc-d576-4560-88c9-5f65141b7f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Polynomial regression is used when the relationship between the independent variable and the dependent variable is not linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d6cb3d-22c3-47ad-bcee-8b927db3ce0f",
   "metadata": {},
   "source": [
    "21. What is a loss function and what is its purpose in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "73b371ad-8bad-4c46-bb4e-b5fc88a2c7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## A loss Function is a performance parameter which shows how well our model predicts the data. \n",
    "##It is used to guide the learning process by telling the model how to update its parameters to minimize the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "424429bc-8609-442c-964c-98ac6ff30c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The purpose of Loss funcion is to update the parameters iteratively till it reaches minimum value.\n",
    "## Some common loss Fucntions are: Mean Squared Error, Mean Absolute Error,cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430d5750-cb90-49ea-a96d-ab8e442e20be",
   "metadata": {},
   "source": [
    "22. What is the difference between a convex and non-convex loss function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6719b23c-3a43-491b-b8f0-14caf5bbae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The main difference between a convex and non-convex loss function is that a convex loss function has only one global minimum, while a non-convex loss function can have multiple local minima.\n",
    "## Another difference between convex and non-convex loss functions is that convex loss functions are easier to optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb2a635-b7c6-454b-b445-64e29d375ef1",
   "metadata": {},
   "source": [
    "23. What is mean squared error (MSE) and how is it calculated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f8986965-ca2a-42a2-8425-49dcf5b0ec27",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MSE is a performance metrics to analyse how good / accurate our model is for  the Regression analysis\n",
    "## The MSE is calculated by squaring the difference between the predicted and actual values for each data point, and then averaging the squared differences\n",
    "## Formula=Î£(yi - yp)^2 / n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4d946f-2900-4b51-8da6-df982cc663a1",
   "metadata": {},
   "source": [
    "24. What is mean absolute error (MAE) and how is it calculated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "82da1dcf-e22f-4bec-a1e2-b65bbd90ea85",
   "metadata": {},
   "outputs": [],
   "source": [
    "## It is also performance metrics for regression problems which tells how much variation is there from actual value from predicted value\n",
    "## The MAE is calculated by taking the absolute value of the difference between the predicted and actual values for each data point, and then averaging the absolute differences\n",
    "## MAE = Î£|yi - yp| / n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9022649e-deba-4f79-8857-f7ffc623e602",
   "metadata": {},
   "source": [
    "25. What is log loss (cross-entropy loss) and how is it calculated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "95fbfd7a-c99a-4490-b4cf-51d19bc86299",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Log loss, also known as cross-entropy loss, is a commonly used loss function in machine learning and optimization problems, particularly in classification tasks.\n",
    "## Log loss = -[y * log(p) + (1 - y) * log(1 - p)]\n",
    "## y represents the true class label (either 0 or 1).\n",
    "#  p is the predicted probability for the true class label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537333f2-51b4-4f4c-b5d6-b2d93edf9569",
   "metadata": {},
   "source": [
    "26. How do you choose the appropriate loss function for a given problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "97ae940e-72e6-401b-800c-0ac1cac240a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Choosing the appropriate loss function for a given problem depends on the nature of the problem, the type of data, and the specific goals of the task\n",
    "## For Eg. MSE and MAE are used for REgression while Log loss is used for classification.\n",
    "## if the goal is to minimize the average error, then MSE is a good choice. However, if the goal is to minimize the probability of misclassification, then log loss is a good choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b53ed12-4459-4076-9d26-77c33983123b",
   "metadata": {},
   "source": [
    "27. Explain the concept of regularization in the context of loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7b0832ae-cc97-4637-a957-3d5c84e6544e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Regularization is a technique used in machine learning to prevent overfitting.\n",
    "## Regularization works by adding a penalty term to the loss function that discourages the model from becoming too complex.\n",
    "## There are two main types of regularization: L1 regularization and L2 regularization. L1 regularization penalizes the absolute values of the model weights, \n",
    "## while L2 regularization penalizes the squared values of the model weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4d3937-0cef-437b-931b-daf2c055f1d3",
   "metadata": {},
   "source": [
    "28. What is Huber loss and how does it handle outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e75a265b-fccc-4489-bcad-db2e8f03c77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Huber loss is a loss function that is used in machine learning to handle outliers. It is a compromise between mean squared error (MSE) and mean absolute error (MAE).\n",
    "## Huber loss handles outliers by employing a delta parameter, denoted as Î´\n",
    "## The choice of the Î´ parameter determines the sensitivity of the loss function to outliers. A larger Î´ value allows for larger residuals to be treated as inliers,\n",
    "## reducing the impact of outliers on the loss. On the other hand, a smaller Î´ value makes the loss function more sensitive to outliers, similar to MSE loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a3551d-085d-4dc4-a70f-6e6598bc1f5a",
   "metadata": {},
   "source": [
    "29. What is quantile loss and when is it used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "deafff0f-7ef5-49dc-949a-b4992b7d4249",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Quantile loss is a loss function that is used in machine learning to predict quantiles. A quantile is a value below which a certain fraction of observations in a group falls.\n",
    "## For example, the 0.5 quantile is the median, and the 0.9 quantile is the 90th percentile.\n",
    "## Quantile loss is useful when we want to capture different levels of uncertainty or variability in the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5703ad40-179f-4395-ad7b-1597582b6158",
   "metadata": {},
   "source": [
    "30. What is the difference between squared loss and absolute loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "af3c601f-2e90-4fe9-bbef-6cda0413f436",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Squared loss measures the error as the squared difference between the predicted values and the actual values\n",
    "## Absolute loss measures the error as the absolute difference between the predicted values and the actual values\n",
    "## Squared loss: If you are trying to predict a continuous value, such as the price of a stock or the height of a person.\n",
    "## Absolute loss: If you are trying to predict a categorical value, such as whether or not a customer will click on an ad or whether or not a patient will recover from a disease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ada9453-5ce8-48af-8307-80ea8af9d33f",
   "metadata": {},
   "source": [
    "31. What is an optimizer and what is its purpose in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "28edb641-003a-4e75-b30a-62aa71599c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "## An optimizer is an algorithm that updates the parameters of a machine learning model in order to minimize a loss function\n",
    "## In machine learning, the parameters of a model are the weights and biases of the model. The weights are the coefficients that multiply the features in the model,\n",
    "## and the biases are the constants that are added to the features. The optimizer updates the weights and biases of the model in order to minimize the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da09111-0724-4547-8bef-ed565218a622",
   "metadata": {},
   "source": [
    "32. What is Gradient Descent (GD) and how does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0b2ae172-97a2-47e5-9030-a07121585a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gradient descent (GD) is an iterative optimization algorithm for finding the minimum of a function.\n",
    "## It works by starting at a random point and then moving in the direction of the steepest descent until it reaches a minimum\n",
    "## GD works by starting with a random set of parameters and then updating the parameters in the direction of the negative gradient of the loss function. \n",
    "## The negative gradient of the loss function points in the direction of the steepest descent, so by moving in this direction, GD is guaranteed to eventually reach a minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf335ec-d05b-4d6f-9f53-9f06b281fdea",
   "metadata": {},
   "source": [
    "33. What are the different variations of Gradient Descent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f72a1491-67be-4959-a666-26eafb179291",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Common Variants\n",
    "## 1.Batch gradient descent:It updates the parameters of the model using the entire training dataset at each step\n",
    "## 2.Stochastic gradient descent: Stochastic gradient descent updates the parameters of the model using a random single data point at each step\n",
    "## 3.Mini-batch gradient descent: It updates the parameters of the model using a small subset of the training dataset at each step\n",
    "## 4.Momentum:Momentum works by adding a fraction of the previous update to the current update\n",
    "## 5.AdaGrad (Adaptive Gradient): AdaGrad adapts the learning rate of each parameter based on their historical gradients. It assigns larger updates to parameters with smaller gradients,\n",
    "##   allowing for more progress in the sparse and steep dimensions of the parameter space. \n",
    "## 6.RMSProp: RMSProp is a variation of AdaGrad that is more stable than AdaGrad. RMSProp works by averaging the gradients\n",
    "##   of the loss function and then using this average to adjust the learning rate\n",
    "## 7.Adam: Adam is a recent optimizer that combines the advantages of AdaGrad and RMSProp. Adam is a very efficient optimizer that is also very stable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d53a10-d436-4de4-85ee-70a7ee554999",
   "metadata": {},
   "source": [
    "34. What is the learning rate in GD and how do you choose an appropriate value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6daff017-95da-45ff-aef6-3e15f49bdae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The learning rate in GD is a hyperparameter that controls how quickly the algorithm converges. A larger learning rate will cause GD to converge more quickly, \n",
    "## but it may also cause GD to overshoot the minimum. A smaller learning rate will cause GD to converge more slowly, but it will be more likely to converge to the minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b8811a37-acd8-462a-a086-3d6586fc79a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here are some tips for choosing an appropriate learning rate:\n",
    "\n",
    "## 1.Start with a small learning rate and then increase it gradually until you find a value that works well.\n",
    "## 2.Use a learning rate decay schedule to reduce the learning rate over time. This will help to prevent GD from overshooting the minimum.\n",
    "## 3.Use a validation set to evaluate the model's performance as you are training it. This will help you to choose a learning rate that minimizes the loss function on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf4df8b-5f68-4bca-b82f-401f6ef18656",
   "metadata": {},
   "source": [
    "35. How does GD handle local optima in optimization problems?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c5588ab0-c54a-4bfc-a7d8-444afd4f1f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "## There are a number of ways to handle local minima in GD. One way is to use a technique called random restarts. \n",
    "## Random restarts involves restarting the GD algorithm from a random point in the search space. This can help the algorithm to escape from local minima and find the global minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1413d573-630d-4db4-af5a-89ab3a6f303f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Another way to handle local minima is to use a technique called momentum. Momentum involves adding a fraction of the previous update to the current update. \n",
    "##This helps to smooth out the updates and prevents GD from getting stuck in local minima"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dc454b-328c-4d34-a9b3-abfc96c1f69e",
   "metadata": {},
   "source": [
    "36. What is Stochastic Gradient Descent (SGD) and how does it differ from GD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "49c511a0-1c84-407b-95f8-dc11bb6d453d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## .Stochastic gradient descent: Stochastic gradient descent updates the parameters of the model using a random single data point at each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8a473fe0-b19d-4eb0-97d8-38d31333037f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## if differs from GD in following ways\n",
    "## Batch size: GD updates the parameters of the model using the entire training dataset at each step. SGD updates the parameters of the model using a single data point at each step.\n",
    "## Accuracy: GD is typically more accurate than SGD.\n",
    "## Convergence: GD can be slow to converge, especially for large problems. SGD is typically faster to converge than GD, but it can be more prone to getting stuck in local minima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a7b997-ea41-47fd-bed5-4334214c1640",
   "metadata": {},
   "source": [
    "37. Explain the concept of batch size in GD and its impact on training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9682fe39-3c23-45bb-b5c3-a947e763035b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## In machine learning or deep learning, batch size is the number of data points that are used to update the model's parameters in each iteration of gradient descent.\n",
    "## A larger batch size will result in more accurate updates, but it will also be more computationally expensive. A smaller batch size will be less computationally expensive,\n",
    "## but it may result in less accurate updates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d356535-baaa-4684-a671-c5f2ddde5e9d",
   "metadata": {},
   "source": [
    "38. What is the role of momentum in optimization algorithms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bb330142-9d5c-42c2-9c9b-20d6bb755519",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Momentum is a technique used in optimization algorithms to improve the convergence of the algorithm. Momentum works by adding a fraction of the previous update to the current update. \n",
    "## This helps to smooth out the updates and prevents the algorithm from getting stuck in local minima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "07c9a700-a5f5-43c3-aecc-7d1f7f47cbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The amount of momentum is typically controlled by a hyperparameter called the momentum coefficient. The momentum coefficient is a value between 0 and 1.\n",
    "## A higher momentum coefficient will result in more momentum, which will make the algorithm converge more quickly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58093d8-747d-4cbe-9b0e-eeb6abb3ac53",
   "metadata": {},
   "source": [
    "39. What is the difference between batch GD, mini-batch GD, and SGD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5377e485-f25e-47df-bae5-94b125b23e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Batch gradient descent updates the model's parameters using the entire training dataset at each step. This makes BGD very accurate, but it can also be very slow, especially for large datasets.\n",
    "## Mini-batch gradient descent updates the model's parameters using a small subset of the training dataset at each step. This makes MBGD faster than BGD, but it can also be less accurate.\n",
    "## Stochastic gradient descent updates the model's parameters using a single data point at each step. This makes SGD the fastest of the three, but it can also be the least accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46011c8b-bb05-4fab-b977-a1b4b5d47898",
   "metadata": {},
   "source": [
    "40. How does the learning rate affect the convergence of GD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1bdb5122-1906-4587-8ab6-ca0454148df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The learning rate is a critical hyperparameter in Gradient Descent (GD) algorithms, and it significantly affects the convergence of the optimization process.\n",
    "## A larger learning rate will cause the model's parameters to be updated more quickly, but it may also cause the model to overshoot the minimum. A smaller learning rate will cause\n",
    "## the model's parameters to be updated more slowly, but it may also cause the model to converge more slowly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b6fb64-8f87-4482-b686-36e382e78d98",
   "metadata": {},
   "source": [
    "41. What is regularization and why is it used in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d936bb4f-290b-4b6f-9119-b9c6e2f6114a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Regularization is a technique used in machine learning to prevent overfitting.\n",
    "## Regularization works by adding a penalty term to the loss function that discourages the model from becoming too complex.\n",
    "## There are two main types of regularization: L1 regularization and L2 regularization. L1 regularization penalizes the absolute values of the model weights, \n",
    "## while L2 regularization penalizes the squared values of the model weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff88a6c8-5f02-452e-99e9-d2c3e259278f",
   "metadata": {},
   "source": [
    "42. What is the difference between L1 and L2 regularization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "86f33942-9bd9-4d42-bdb3-fe9b0f13ef4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## L1 regularization penalizes the absolute values of the model weights, \n",
    "## while L2 regularization penalizes the squared values of the model weights\n",
    "## L1 regularization encourages sparsity because it penalizes the absolute values of the parameters. \n",
    "## This means some parameteres becomes 0 while in L2 regularization encourages small values because it penalizes the square of the parameters but does not become 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139720df-87d0-4753-8c62-12ac9401f4b8",
   "metadata": {},
   "source": [
    "43. Explain the concept of ridge regression and its role in regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "02d08040-d86d-4fba-ae37-10f958d3465c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ridge regression is a regularized linear regression model that penalizes the sum of the squared coefficients of the model.\n",
    "## This helps to prevent overfitting, which is a problem that occurs when a model learns the training data too well and is unable to generalize to new data\n",
    "## L2 regularization adds a penalty to the loss function that is proportional to the square of the model's parameters. \n",
    "## This encourages the model to have smaller parameters, which can help to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf11911-1762-404c-b86b-7431b6b231b8",
   "metadata": {},
   "source": [
    "44. What is the elastic net regularization and how does it combine L1 and L2 penalties?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0a5b9fe2-68af-4bb7-abfc-2763ddac656a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Elastic net regularization is a regularized machine learning model that combines L1 and L2 regularization\n",
    "## Elastic net regularization combines these two penalties by adding a weighted sum of the L1 and L2 penalties to the loss function. \n",
    "## The weights of the L1 and L2 penalties are typically denoted by Î± and Î², respectively\n",
    "## If Î± is large, then the L1 penalty will be more important. If Î² is large, then the L2 penalty will be more important"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac950d71-b248-42a1-ae49-9026bd65bc25",
   "metadata": {},
   "source": [
    "45. How does regularization help prevent overfitting in machine learning models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e1202748-38b6-4641-ad61-c670161a8618",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Regularization is a technique that can be used to prevent overfitting in machine learning models\n",
    "## Regularization works by adding a penalty to the loss function of the model\n",
    "## This penalty is typically proportional to the size of the model's parameters. The larger the penalty, the smaller the parameters will be. \n",
    "## This helps to prevent the model from becoming too complex and from overfitting the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1436f7e5-6600-4ff9-82e5-e560379d6112",
   "metadata": {},
   "source": [
    "46. What is early stopping and how does it relate to regularization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "73bb0e77-90b0-4001-a22c-d1c4e92c738a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Early stopping is a technique that can be used to prevent overfitting in machine learning models\n",
    "## Early stopping works by stopping the training of the model early, before it has had a chance to overfit the training data. \n",
    "## This is done by monitoring the performance of the model on a validation dataset\n",
    "## If the performance of the model on the validation dataset starts to decrease, then the training is stopped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8737c576-07a7-4da0-837f-16816e985533",
   "metadata": {},
   "source": [
    "47. Explain the concept of dropout regularization in neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dbe63cc2-4882-489b-896e-a2550e681ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dropout regularization works by randomly dropping out some of the neurons in the neural network during training. \n",
    "## This means that some of the neurons will not be used to compute the output of the neural network. This forces the neural network to rely on the other neurons\n",
    "## to make predictions, which helps to prevent the neural network from becoming too complex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4099d1-b0ca-4de9-9420-d6a8d0deb1ee",
   "metadata": {},
   "source": [
    "48. How do you choose the regularization parameter in a model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9c8c1ba0-cd56-4fca-aee1-0e26fac36b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Manaul Selection:Start by trying a range of Î» values, such as 0.001, 0.01, 0.1, 1, 10, etc.\n",
    "## GridSearch:Define a grid of possible Î» values, including a range of small and large values\n",
    "## Random Search:Instead of exhaustively searching a predefined grid, randomly sample Î» values from a given range"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cbaca1-f57b-4673-a91c-4725697a6b28",
   "metadata": {},
   "source": [
    "49. What is the difference between feature selection and regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2861c64b-8f25-439d-bde2-292e5559c654",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature selection is a process of selecting the most important features from a dataset\n",
    "## This can be done by using a variety of techniques, such as statistical tests, information gain, and correlation analysis.\n",
    "## Feature Selection improve model performance and accuracy\n",
    "## Regularization is a technique that adds a penalty to the loss function of the model.\n",
    "## REgularization makes some parameters coefiicients to zero where it can be used for feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc53b221-f8a2-4b19-a90b-f26c29d23a9b",
   "metadata": {},
   "source": [
    "50. What is the trade-off between bias and variance in regularized models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "53bc7f46-26c0-44c0-baeb-43816f69481c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## In machine learning, bias and variance are two measures of the accuracy of a model. Bias is the difference between the expected value of the model's predictions \n",
    "## and the true value of the target variable.Variance is the variability of the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d3ce06f9-9dd3-4c6d-a13b-459a82b1e7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## If the algorithm is too simple (hypothesis with linear equation) then it may be on high bias and low variance condition and thus is error-prone. \n",
    "## If algorithms fit too complex (hypothesis with high degree equation) then it may be on high variance and low bias.\n",
    "## The bias-variance trade-off can be visualized as an inverted U-shaped curve. As the level of regularization increases from zero, \n",
    "## the model's complexity decreases, resulting in an increase in bias and a decrease in variance\n",
    "## The goal is to find the optimal level of regularization that minimizes both bias and variance,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dfe082-3832-4c70-b782-678c8c3243af",
   "metadata": {},
   "source": [
    "51. What is Support Vector Machines (SVM) and how does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "57247d85-9f0b-4978-bebf-a8cbbdddc645",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Support vector machines (SVM) are a powerful machine learning algorithm that can be used for classification and regression tasks.\n",
    "## SVMs work by finding the hyperplane that best separates the two classes of data. The hyperplane is a line or a plane that divides the data into two regions, \n",
    "## such that all the points in one region belong to one class and all the points in the other region belong to the other class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d235c454-f3f6-4fa7-b457-e86a43dada7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The goal of SVMs is to find the hyperplane that maximizes the margin between the two classes of data. The margin is the distance between the hyperplane and the nearest points of each class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37e8735-889d-4f01-957f-0349860d9515",
   "metadata": {},
   "source": [
    "52. How does the kernel trick work in SVM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "872a66da-e311-4025-b3d7-49615a7ac727",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The kernel trick is a technique used in support vector machines (SVMs) to map data into a higher dimensional space where the data becomes linearly separable.\n",
    "## This allows SVMs to be used for tasks where the data is not linearly separable in the original space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1b9f833c-1c8e-4e6e-8837-fca71e1ef752",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The kernel trick works by applying a kernel function to the data. The kernel function is a mathematical function that maps the data into a higher dimensional space.\n",
    "## The most common kernel function used in SVMs is the Gaussian kernel, which is also known as the radial basis function kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fdd0f1-1a7d-456b-89a7-8d427a917464",
   "metadata": {},
   "source": [
    "53. What are support vectors in SVM and why are they important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "340594d8-5c45-45f0-a350-ce93e2c367c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## In support vector machines (SVM), support vectors are the points that are closest to the hyperplane that separates the two classes of data\n",
    "## The support vectors are important because they determine the position of the hyperplane. The hyperplane is defined as the line or plane that passes through the support vectors\n",
    "## and maximizes the margin between the two classes of data\n",
    "## The larger the margin, the better the SVM will be at generalizing to new data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c3bb19-290f-420d-9758-e25b229e3313",
   "metadata": {},
   "source": [
    "54. Explain the concept of the margin in SVM and its impact on model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "27f78981-e5e3-477f-a90a-4ca3ef787bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## In support vector machines (SVM), the margin is the distance between the hyperplane and the nearest points of each class.\n",
    "## ## The larger the margin, the better the SVM will be at generalizing to new data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c6934e-c933-4849-a79f-593a7d1127fa",
   "metadata": {},
   "source": [
    "55. How do you handle unbalanced datasets in SVM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "755f9204-f11a-424f-96d3-80867972a109",
   "metadata": {},
   "outputs": [],
   "source": [
    "## There are various ways which can be used to handle imbalanced datasets\n",
    "## Cost-sensitive learning:Cost-sensitive learning assigns different costs to misclassifications of different classes. \n",
    "## This allows the model to focus on reducing the number of misclassifications of the minority class.\n",
    "## Undersampling:Undersampling reduces the size of the majority class by randomly removing samples.\n",
    "## Overampling:Oversampling increases the size of the minority class by randomly duplicating samples. \n",
    "## Smote:SMOTE (Synthetic Minority Over-sampling Technique) create synthetic samples by interpolating between existing minority class samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ddb344-d008-4162-8705-93b8f2529c3c",
   "metadata": {},
   "source": [
    "56. What is the difference between linear SVM and non-linear SVM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3e50c784-31c2-44bc-8c80-6fd7642e103c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Linear SVMs can only find linear hyperplanes. This means that the data must be linearly separable in order for a linear SVM to be able to find a hyperplane that separates the two classes\n",
    "## Non-linear SVMs can find non-linear hyperplanes. This means that the data does not need to be linearly separable in order for a non-linear SVM to be able \n",
    "## to find a hyperplane that separates the two classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "51d597a2-7353-48af-b014-0ac89bd96d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Non-linear SVMs use a kernel trick to map the data into a higher dimensional space where the data becomes linearly separable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568ea7bf-9c06-4584-b7e3-aabc64cec2a8",
   "metadata": {},
   "source": [
    "57. What is the role of C-parameter in SVM and how does it affect the decision boundary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c739d33e-f93b-4994-b7fe-23c02dac16bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## C-parameter controls the trade-off between the margin and the number of support vectors.\n",
    "## The C-parameter controls this trade-off by determining how much penalty is given to misclassified points. A larger C-parameter means that more penalty is given to \n",
    "## misclassified points, which leads to a smaller margin and more support vectors. A smaller C-parameter means that less penalty is given to misclassified points, \n",
    "## which leads to a larger margin and fewer support vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "282167eb-fb6d-4c34-906e-5e9f20bfc6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The C-parameter affects the decision boundary by controlling the position of the hyperplane. A larger C-parameter means that the hyperplane will be closer to the \n",
    "## nearest points of each class, which leads to a narrower decision boundary. A smaller C-parameter means that the hyperplane will be further away from the nearest points\n",
    "## of each class, which leads to a wider decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533cc618-961f-4964-bf09-714e1a0a9049",
   "metadata": {},
   "source": [
    "58. Explain the concept of slack variables in SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a9147d61-abfc-482c-a8be-1df41d1329ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## In support vector machines (SVM), slack variables are used to relax the constraints of the optimization problem\n",
    "## The optimization problem for SVM is to find a hyperplane that maximizes the margin between the two classes of data.\n",
    "## Slack variables are added to the optimization problem to allow some points to be misclassified.This makes the optimization problem easier to solve\n",
    "## The larger the value of the slack variable, the further the point is from the hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b44463f-c7ec-4911-bc99-1d74c26ab0e8",
   "metadata": {},
   "source": [
    "59. What is the difference between hard margin and soft margin in SVM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ecf488c9-9e78-4def-8604-25c27e33e2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hard margin SVMs require all training data points to be perfectly classified, while soft margin SVMs allow some training data points to be misclassified.\n",
    "## Hard margin SVMs are more sensitive to noise than soft margin SVMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7392d8de-5ec8-4445-8477-ea322ec8926f",
   "metadata": {},
   "source": [
    "60. How do you interpret the coefficients in an SVM model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5f9a1b09-37b1-445d-a887-40509b8085ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## In support vector machines (SVM), the coefficients in an SVM model represent the importance of each feature in the model.\n",
    "## The coefficients are calculated during the training process, and they are used to determine the position of the hyperplane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7e8d5d0a-1f1b-475f-bbd5-d4d38fc07f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "## A positive coefficient indicates that the feature is positively correlated with the class label\n",
    "## A negative coefficient indicates that the feature is negatively correlated with the class label\n",
    "## The magnitude of the coefficient indicates the importance of the feature. The larger the magnitude of the coefficient, the more important the feature is in determining the class label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e54a0e9-d413-4323-aec4-36bbc6326948",
   "metadata": {},
   "source": [
    "61. What is a decision tree and how does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b71fc865-52d3-41bf-afdd-d524d0c12f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "## A decision tree is a supervised learning algorithm that can be used for classification and regression tasks. \n",
    "## Decision trees work by breaking down the data into smaller and smaller subsets until a decision can be made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1cabb8a8-f35f-4ebb-9c1f-89f28c4b1a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## It is a flowchart-like structure where each internal node represents a feature or attribute, each branch represents a decision rule,\n",
    "## and each leaf node represents the outcome or the predicted value.The decision tree is built by recursively splitting the data into smaller and smaller subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ea5c0008-ceef-4b7f-bb2d-7313550e271e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The splitting process is based on a decision rule. The decision rule is a mathematical expression that is used to determine which subset a data point should be placed in.\n",
    "## The decision rule is typically based on the value of one or more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d93d09ce-8e44-49f1-aec9-4d749f450279",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The decision tree is built by repeatedly splitting the data until a stopping criterion is met. The stopping criterion can be based on the number of data points in the subset,\n",
    "## the homogeneity of the subset, or the depth of the tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f7284b-a6b5-4c4f-b209-f3f790eaf326",
   "metadata": {},
   "source": [
    "62. How do you make splits in a decision tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c98e550a-2045-43b0-a7f0-751e2595121e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## There are two main ways to make splits in a decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2fecca95-af4a-497a-8a2d-e3cef2eba72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gini impurity: The Gini impurity is a measure of how mixed the classes are in a particular subset. The lower the Gini impurity, \n",
    "##   the more homogeneous the subset is. The decision rule that minimizes the Gini impurity is chosen as the best split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "08614ca3-35ea-4cbd-ae32-32585f831ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Information gain: Information gain is a measure of how much information is gained by splitting a subset. \n",
    "## The higher the information gain, the more information is gained by splitting the subset.The decision rule that maximizes the information gain is chosen as the best split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e471bd-7653-41b2-97c5-e898b9b31eca",
   "metadata": {},
   "source": [
    "63. What are impurity measures (e.g., Gini index, entropy) and how are they used in decision trees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "667cba58-a999-4d50-a298-abac9ff81294",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Impurity measures are used in decision trees to evaluate the homogeneity of a dataset. They are a measure of how mixed the classes are in a particular subset.\n",
    "## The lower the impurity, the more homogeneous the subset is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5524ab17-0d8c-486c-b597-82bcbde47219",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gini index: The Gini index is a measure of how likely it is that a randomly chosen data point from a particular subset will be misclassified.\n",
    "##             The lower the Gini index, the less likely it is that a randomly chosen data point will be misclassified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "db4d0f2a-c6d8-4ece-ba35-a1e4930fdd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Entropy: Entropy is a measure of the uncertainty of a particular subset. The higher the entropy, the more uncertain the subset is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856aeb35-a5e0-4f15-87ef-074c10f95d73",
   "metadata": {},
   "source": [
    "64. Explain the concept of information gain in decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2f23c2d2-5a1b-4783-9815-14fe7bad4615",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Information gain is a measure of how much information a feature provides about a class. In decision trees, it is used to determine the best way to split a node. \n",
    "## The feature with the highest information gain is the one that will most effectively reduce the uncertainty in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fd9ea1-46f5-4516-82bf-0e164a2d1d5f",
   "metadata": {},
   "source": [
    "65. How do you handle missing values in decision trees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "318b132e-b23d-4a28-8c66-46286548ca06",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The best way to handle missing values in decision trees depends on the specific dataset and the desired accuracy. If the dataset is small and the missing values are rare, \n",
    "## then ignoring the missing values may be a reasonable approach.\n",
    "## However, if the dataset is large or the missing values are common, \n",
    "## then it may be necessary to use a more sophisticated method, such as imputation or a decision tree algorithm that is designed to handle missing values.\n",
    "## Some decision tree algorithms, such as CART, can automatically handle missing values. These algorithms will create a separate branch for data points with missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f5d239-c458-45e1-831e-d82c50b16d7f",
   "metadata": {},
   "source": [
    "66. What is pruning in decision trees and why is it important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "87846dc5-15e6-4e53-bd5c-fb1fa6f4125c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pruning is a technique used to reduce the size of a decision tree by removing sections of the tree that are non-critical and redundant to classify instances. \n",
    "## Pruning reduces the complexity of the final classifier, and hence improves predictive accuracy by the reduction of overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b2a085-b2a7-4e24-877e-7f9b4f5ba883",
   "metadata": {},
   "source": [
    "67. What is the difference between a classification tree and a regression tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a24ec476-c1a5-4fc6-93d1-1b152365411d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classification Tree is used for categorical output like yes or no while Regression tree is used for producing continuous Numerical Value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7d775d-e148-41c6-ac18-fcdc762dd9cb",
   "metadata": {},
   "source": [
    "68. How do you interpret the decision boundaries in a decision tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bd381065-bfa7-45b9-88fb-34c0f52d558a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## To interpret the decision boundaries in a decision tree, you can look at the rules that are used to split the data. These rules are typically based on the values of the features.\n",
    "## Look at the visualization of the decision tree. This visualization will show you the different decision boundaries and how they are used to classify the data\n",
    "## Consider the size of the decision boundaries. Larger decision boundaries indicate that the features are not very important for predicting the class of a data point.\n",
    "## Smaller decision boundaries indicate that the features are more important for predicting the class of a data point.\n",
    "## Consider the number of decision boundaries. A large number of decision boundaries indicates that the data is complex and that the decision tree is trying to capture a lot of detail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7f3d10-a84e-4915-acfc-c52ed911fa59",
   "metadata": {},
   "source": [
    "69. What is the role of feature importance in decision trees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3a806774-8905-4ea4-ac1d-0be6d1241b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature importance is a measure of how important a feature is for predicting the target variable in a decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "15f17201-f7d8-4ded-86ae-cf0c9d57f857",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hihger is the value the more prominent feature it is for predicting the target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cec2ad4-dbf1-44ba-88ea-d80ade4f8041",
   "metadata": {},
   "source": [
    "70. What are ensemble techniques and how are they related to decision trees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "746639f3-9d68-4c1a-8578-01fbbcac38f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ensemble Technique is a process by which multiple base/weak models are combined together to get a strong model.\n",
    "## In case of Random Forest we use multiple Decision Trees are combined to get a more strong model. It is also used to over the problem of overfitting sing ensemble techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea7ae7f-88ed-4d01-b79a-fa7ad3b1a702",
   "metadata": {},
   "source": [
    "71. What are ensemble techniques in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "07c25e83-3062-44f9-a48f-1b97b6d929dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ensemble Technique is a process by which multiple base/weak models are combined together to get a strong model.\n",
    "## In Regression Problem we take the average of all base models to get the more accurate model\n",
    "## In classification base models output are voted to get a strong model\n",
    "## Some common ensembel techniques are :\n",
    "## Bagging and Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d3b34c-51e4-464d-bf55-34eedc8e954e",
   "metadata": {},
   "source": [
    "72. What is bagging and how is it used in ensemble learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "055af702-4fc8-4738-a31b-f35b44a24747",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bagging is a technique that creates multiple models by sampling the training data with replacement. The predictions of the individual models are then combined to create a single prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "97467c20-b46b-4d81-b03a-318e0628ccc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bagging can be used to improve the accuracy of machine learning models in several ways. First, bagging can help to reduce overfitting.\n",
    "## bagging can improve the accuracy of machine learning models by combining the strengths of multiple models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaadda2-7add-4244-a16f-0aec98a2312f",
   "metadata": {},
   "source": [
    "73. Explain the concept of bootstrapping in bagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "42b67b1b-b217-41d7-8f27-89270fbc0932",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Bootstrapping is a technique used in bagging to create multiple bootstrap samples of the training data. A bootstrap sample is a random sample of the training data with replacement,\n",
    "## which means that some data points may be included in the sample more than once"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255c97b2-d3d5-4caa-935c-ca105f5982cd",
   "metadata": {},
   "source": [
    "74. What is boosting and how does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c25e34e0-bf7e-495a-ab1b-2d20c7a1ff96",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Boosting is an ensemble learning method that combines multiple weak learners to create a strong learner.\n",
    "## Boosting works by assigning higher weights to weak classified data.\n",
    "## Boosting works by sequentially adding weak learners to the ensemble. The first weak learner is trained on the entire training data. The second weak learner is then trained on the training data,\n",
    "## but the weights of the data points are adjusted so that the weak learner focuses on the data points that were misclassified by the first weak learner.\n",
    "## This process is repeated until the desired number of weak learners has been added to the ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e06a8ec-5e72-4a1d-b4d4-a643acadc4ab",
   "metadata": {},
   "source": [
    "75. What is the difference between AdaBoost and Gradient Boosting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2e0ccefc-a0e4-4d0f-b9a1-69da79489130",
   "metadata": {},
   "outputs": [],
   "source": [
    "## AdaBoost works by sequentially adding weak learners to the ensemble, and weighting the predictions of the weak learners according to their accuracy.\n",
    "## The weights of the weak learners are adjusted so that the weak learners that are more accurate are given more weight. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "baaafb75-f30d-4d02-946b-6b1213ba270c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gradient boosting, on the other hand, works by fitting a sequence of decision trees to the residual errors of the previous trees\n",
    "## The residual errors are the difference between the predicted values and the actual values\n",
    "## The decision trees are fitted in a way that minimizes the residual errors. This process is repeated until the desired number of trees has been fitted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86006d8c-6c4d-4219-a9ff-6d11963cd87f",
   "metadata": {},
   "source": [
    "76. What is the purpose of random forests in ensemble learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "588868b6-6fbd-45d1-8639-88938fb0c199",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random forests are a type of ensemble learning algorithm that combines multiple decision trees to create a more accurate and robust model. Random forests are often used for classification and regression tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "da15539d-4317-4e0f-800f-4154efc9c5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The purpose of random forests in ensemble learning is to reduce overfitting and improve the accuracy of the model\n",
    "##  Random forests are more robust to noise and outliers than single decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc23b92-3c9a-4fe1-92c5-e95516f28ac7",
   "metadata": {},
   "source": [
    "77. How do random forests handle feature importance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "60ba4a31-bc0a-454a-908a-1932e43f90ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random forests handle feature importance by measuring the decrease in impurity that is achieved by splitting the data on a particular feature.\n",
    "## The feature with the highest average decrease in impurity is considered to be the most important feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab80e66d-d22c-4b25-b9f8-3de4a2a90e95",
   "metadata": {},
   "source": [
    "78. What is stacking in ensemble learning and how does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0a0dc781-1007-4d4e-bae8-24490ba7aa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stacking is an ensemble learning technique that combines multiple models in a hierarchical manner to create a more accurate and robust model\n",
    "## Stacking works by first training a set of base models on the training data. The predictions of the base models are then used to train a meta-model. The meta-model is then used to make predictions on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91e8553-2499-4ef2-a79f-41c4cdd12f72",
   "metadata": {},
   "source": [
    "79. What are the advantages and disadvantages of ensemble techniques?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b3fb56a4-927d-4f62-9c4e-423429c207ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Advantages:\n",
    "## Improved accuracy: Ensemble techniques can often achieve higher accuracy than single models.\n",
    "## Robustness: Ensemble techniques can be more robust to noise and outliers than single models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7a808e91-7432-4b32-8849-be5ec8ec5aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Disadvantes:\n",
    "## More complex:Ensemble techniques can be more complex than single models. This can make them more difficult to train and deploy.\n",
    "## Computational cost: Ensemble techniques can be more computationally expensive than single models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb15efa-ff87-421f-a371-beb43a10c9f5",
   "metadata": {},
   "source": [
    "80. How do you choose the optimal number of models in an ensemble?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a59270c7-51b5-4a59-b4c1-17df1662fc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Increase the number of models gradually:Once you have a baseline model, you can start increasing the number of models gradually.\n",
    "## You can do this by adding one model at a time and evaluating the performance of the ensemble after each addition.\n",
    "## Hyperparamter Tuning: Use Hyperparamter  to check for the range of models\n",
    "## Cross validation: This helps you to analyse which number of models give best performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8268dc9-d187-4d3d-a836-090cd84743aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
